{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f11ab93",
   "metadata": {},
   "source": [
    "# Módulo 1: Web Scraping de Detalhes dos Anúncios do Airbnb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7006109",
   "metadata": {},
   "source": [
    "## 1. O Ponto de Partida: Um Dataset Incompleto\n",
    "\n",
    "No estágio anterior de nosso projeto, executamos um script de extração (`2 - exemplo_script_extracao_anuncios.py`) que varreu as páginas de busca do Airbnb, coletando informações gerais sobre milhares de anúncios, como ID, título, preço por período e o link para a página principal.\n",
    "\n",
    "O resultado foi um arquivo CSV robusto, porém, incompleto. Para uma análise mais profunda e para popular nosso banco de dados corretamente, precisamos de detalhes que só existem *dentro* da página de cada anúncio.\n",
    "\n",
    "### O Desafio\n",
    "Como podemos, de forma automática e eficiente, visitar milhares de links para extrair informações específicas como o número de quartos, camas, banheiros e as regras de check-in/checkout? Fazer isso manualmente seria impossível.\n",
    "\n",
    "### A Solução: Um Robô de Extração\n",
    "Este notebook é a solução. Ele atua como um \"robô\" (web scraper) que foi programado para realizar as seguintes tarefas:\n",
    "1.  Ler a lista de anúncios do nosso arquivo CSV.\n",
    "2.  Visitar, um por um, o link de cada anúncio único.\n",
    "3.  \"Ler\" a página e extrair de forma inteligente os detalhes que faltam.\n",
    "4.  Salvar esses novos dados de forma estruturada para uso futuro.\n",
    "\n",
    "Vamos começar!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222302ab",
   "metadata": {},
   "source": [
    "## 2. Preparando o Ambiente: As Ferramentas Necessárias\n",
    "\n",
    "Antes de construirmos nosso robô, precisamos importar nossa \"caixa de ferramentas\" de bibliotecas Python. Cada uma tem um papel fundamental:\n",
    "\n",
    "* **Pandas**: Para ler e organizar nossos dados em tabelas (DataFrames).\n",
    "* **Selenium**: A biblioteca principal que nos permite controlar um navegador de internet (Firefox, neste caso) através de código.\n",
    "* **BeautifulSoup**: Uma ferramenta fantástica para \"ler\" o HTML de uma página web e encontrar as informações que queremos de forma fácil.\n",
    "* **OS, Sys, Time, Re**: Bibliotecas auxiliares para interagir com arquivos e pastas, controlar o tempo e encontrar padrões em textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d87e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bibliotecas para Manipulação de Dados e Arquivos ---\n",
    "import pandas as pd  # Usado para criar e manipular DataFrames (nossas tabelas de dados).\n",
    "import os            # Usado para interagir com o sistema operacional, como verificar se um arquivo existe.\n",
    "import sys           # Permite interagir com o sistema, usado aqui para parar o script em caso de erro.\n",
    "import re            # Biblioteca de expressões regulares, para encontrar padrões em textos.\n",
    "import time          # Usado para adicionar pausas (esperas) no código, essenciais em web scraping.\n",
    "\n",
    "# --- Bibliotecas para Web Scraping e Automação ---\n",
    "from selenium import webdriver  # A ferramenta principal que controla o navegador.\n",
    "from selenium.webdriver.firefox.options import Options  # Permite configurar as opções do navegador (ex: modo headless).\n",
    "from selenium.webdriver.common.by import By  # Usado para especificar como encontrar elementos na página (por ID, XPath, etc.).\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # Permite criar esperas inteligentes, aguardando que uma condição aconteça.\n",
    "from selenium.webdriver.support import expected_conditions as EC  # Contém as condições para as esperas inteligentes (ex: elemento visível).\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException  # Classes de erro específicas do Selenium para lidar com imprevistos.\n",
    "from bs4 import BeautifulSoup  # Biblioteca para analisar (parse) o código HTML da página e facilitar a extração de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa324ee",
   "metadata": {},
   "source": [
    "## 3. Construindo o Coração do Robô: A Função de Extração\n",
    "\n",
    "Agora, vamos ao código mais importante deste notebook. A função `extrair_detalhes_anuncio` é o \"cérebro\" do nosso robô. É aqui que ensinamos a ele exatamente como se comportar ao visitar uma página do Airbnb.\n",
    "\n",
    "Pense nesta função como o \"manual de instruções\" do robô para uma única tarefa: ao receber um link, ele deve saber exatamente onde procurar por \"quartos\", \"camas\", \"banheiros\" e as regras da casa. Como as páginas podem mudar, programamos múltiplos métodos de busca (um principal e um alternativo) para torná-lo mais resiliente a erros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8575f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_detalhes_anuncio(driver, url):\n",
    "    \"\"\"\n",
    "    Navega para a URL de um anúncio e extrai detalhes como número de quartos,\n",
    "    camas, banheiros e horários de check-in/check-out.\n",
    "    \"\"\"\n",
    "    # Limpa a URL para garantir que estamos acessando a página principal do anúncio, removendo parâmetros de busca.\n",
    "    base_url = url.split('?')[0].split('/house-rules')[0]\n",
    "\n",
    "    # Inicializa as variáveis como 'None'. Isso garante que, se uma extração falhar, o valor será nulo e não um erro.\n",
    "    quartos = None\n",
    "    camas = None\n",
    "    banheiros = None\n",
    "    horario_checkin = None\n",
    "    horario_checkout = None\n",
    "\n",
    "    # --- Bloco 1: Extração de detalhes da acomodação (Quartos, Camas, Banheiros) ---\n",
    "    print(\"  - Extraindo Quartos, Camas e Banheiros...\")\n",
    "    try:\n",
    "        # Define um tempo máximo de 20 segundos para a página carregar. Se demorar mais, lança um erro (TimeoutException).\n",
    "        driver.set_page_load_timeout(20)\n",
    "        # O navegador acessa a URL do anúncio.\n",
    "        driver.get(base_url)\n",
    "        # Aguarda de forma inteligente (por até 20s) que a tag <body> da página esteja presente, indicando que o HTML básico foi carregado.\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        # Adiciona uma pausa estática de 3 segundos para garantir que scripts dinâmicos (Javascript) tenham tempo de renderizar o conteúdo.\n",
    "        time.sleep(3) \n",
    "\n",
    "        # Pega o código-fonte HTML completo da página após o carregamento e o entrega ao BeautifulSoup para análise.\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Tentativa 1 (Método Principal): Busca a seção de \"visão geral\" que geralmente contém os detalhes.\n",
    "        overview_section = soup.find('div', {'data-plugin-in-point-id': 'OVERVIEW_DEFAULT_V2'})\n",
    "        if overview_section:\n",
    "            # Se a seção for encontrada, busca todos os itens de lista ('li') dentro dela.\n",
    "            lista_itens = overview_section.find_all('li', class_='l7n4lsf')\n",
    "            # Itera sobre cada item encontrado.\n",
    "            for item in lista_itens:\n",
    "                # Limpa o texto do item, removendo caracteres como '·' e espaços em branco.\n",
    "                texto_item = item.get_text(strip=True).replace('·', '').strip()\n",
    "                # Verifica se o texto contém palavras-chave e atribui à variável correta.\n",
    "                if 'quarto' in texto_item:\n",
    "                    quartos = texto_item\n",
    "                elif 'cama' in texto_item:\n",
    "                    camas = texto_item\n",
    "                elif 'banheiro' in texto_item:\n",
    "                    banheiros = texto_item\n",
    "        else:\n",
    "            # Tentativa 2 (Método Alternativo): Se o método principal falhar (pois o site pode mudar), este é executado.\n",
    "            print(\"    Seção de visão geral não encontrada. Tentando método alternativo.\")\n",
    "            try:\n",
    "                # Localiza um texto de referência (\"hóspedes\") para encontrar a seção correta.\n",
    "                overview_element = driver.find_element(By.XPATH, \"//*[contains(text(), 'hóspedes')]\")\n",
    "                # A partir do texto, navega para o elemento \"pai\" que contém todos os detalhes.\n",
    "                parent_div = overview_element.find_element(By.XPATH, \"./..\")\n",
    "                # Pega todos os textos dentro do elemento pai.\n",
    "                items = parent_div.find_elements(By.TAG_NAME, 'span')\n",
    "                # Junta todos os pedaços de texto em uma única string.\n",
    "                full_text = ' '.join([item.text for item in items if item.text.strip()])\n",
    "\n",
    "                # Usa expressões regulares (regex) para extrair os padrões exatos que queremos.\n",
    "                q = re.search(r'(\\d+\\s*quarto|Estúdio)', full_text)\n",
    "                c = re.search(r'(\\d+\\s*cama)', full_text)\n",
    "                b = re.search(r'(\\d+\\s*banheiro)', full_text)\n",
    "                # Se um padrão for encontrado (match), armazena o resultado.\n",
    "                if q: quartos = q.group(1)\n",
    "                if c: camas = c.group(1)\n",
    "                if b: banheiros = b.group(1)\n",
    "            except Exception:\n",
    "                # Se o método alternativo também falhar, ele apenas ignora o erro e continua.\n",
    "                pass \n",
    "\n",
    "    except TimeoutException:\n",
    "        # Este erro é capturado se a página demorar mais que o tempo limite para carregar.\n",
    "        print(\"  - A página principal demorou muito para carregar. Pulando extração de quartos/camas/banheiros.\")\n",
    "    except Exception as e:\n",
    "        # Captura qualquer outro erro inesperado para evitar que o script pare.\n",
    "        print(f\"  ERRO ao extrair quartos/camas/banheiros: {e}\")\n",
    "        # Atribui um valor de 'Erro' para sabermos que a extração falhou para este item.\n",
    "        quartos, camas, banheiros = 'Erro', 'Erro', 'Erro'\n",
    "\n",
    "    # Imprime os resultados encontrados para acompanhamento em tempo real no console.\n",
    "    print(f\"    Quartos: {quartos}, Camas: {camas}, Banheiros: {banheiros}\")\n",
    "\n",
    "    # --- Bloco 2: Extração de Horários de Check-in e Check-out ---\n",
    "    print(\"  - Extraindo Check-in/Check-out...\")\n",
    "    try:\n",
    "        # Cria um novo 'soup' para garantir que temos o estado mais atualizado da página.\n",
    "        soup_regras = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Tentativa 1: Busca a seção de políticas da casa diretamente.\n",
    "        policies_section = soup_regras.find('div', {'data-section-id': 'POLICIES_DEFAULT'})\n",
    "        if policies_section:\n",
    "            rule_items = policies_section.find_all('div', class_='i1303y2k')\n",
    "            for item in rule_items:\n",
    "                texto_item = item.get_text(strip=True)\n",
    "                if 'Check-in' in texto_item:\n",
    "                    horario_checkin = texto_item\n",
    "                elif 'Checkout' in texto_item:\n",
    "                    horario_checkout = texto_item\n",
    "\n",
    "        # Tentativa 2 (Fallback): Se as regras não estavam visíveis, tenta clicar no botão \"Mostrar mais\".\n",
    "        if not horario_checkin or not horario_checkout:\n",
    "            print(\"    Check-in/Check-out não encontradas, tentando clicar em 'Mostrar mais'...\")\n",
    "            try:\n",
    "                # Aguarda por até 5 segundos que o botão se torne clicável.\n",
    "                show_more_button = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, \"//a[contains(., 'Mostrar regras da casa')] | //button[contains(., 'Mostrar mais')]\"))\n",
    "                )\n",
    "                # Clica no botão usando JavaScript, uma técnica mais robusta.\n",
    "                driver.execute_script(\"arguments[0].click();\", show_more_button)\n",
    "                # Espera 2 segundos para a janela (modal) com as regras abrir.\n",
    "                time.sleep(2)\n",
    "\n",
    "                # Analisa o novo código HTML da página, agora com o modal aberto.\n",
    "                soup_modal = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                modal_rule_items = soup_modal.find_all('div', class_='f15dgkuj')\n",
    "                for item in modal_rule_items:\n",
    "                    texto_item = item.get_text(strip=True)\n",
    "                    if 'Check-in:' in texto_item:\n",
    "                        horario_checkin = texto_item\n",
    "                    elif 'Checkout:' in texto_item:\n",
    "                        horario_checkout = texto_item\n",
    "            except (TimeoutException, NoSuchElementException):\n",
    "                # Erro comum se o botão não existir ou não aparecer a tempo.\n",
    "                print(\"    Botão 'Mostrar mais' para Check-in/Check-out não encontrado.\")\n",
    "            except Exception as e:\n",
    "                # Captura de outros erros possíveis ao interagir com o modal.\n",
    "                print(f\"    Erro ao tentar abrir o modal de Check-in/Check-out: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Captura de erro geral para este bloco.\n",
    "        print(f\"  ERRO FATAL ao extrair os horários de check-in/out: {e}\")\n",
    "        horario_checkin, horario_checkout = 'Erro na extração', 'Erro na extração'\n",
    "    finally:\n",
    "        # Garante que o tempo de espera do driver seja restaurado para o padrão (60s), evitando problemas na próxima iteração.\n",
    "        driver.set_page_load_timeout(60)\n",
    "\n",
    "    # Imprime os resultados para acompanhamento.\n",
    "    print(f\"    Check-in: {horario_checkin}\")\n",
    "    print(f\"    Check-out: {horario_checkout}\")\n",
    "\n",
    "    # Retorna um dicionário com todos os dados coletados.\n",
    "    return {\n",
    "        \"Quartos\": quartos,\n",
    "        \"Camas\": camas,\n",
    "        \"Banheiros\": banheiros,\n",
    "        \"Horário de Check-in\": horario_checkin,\n",
    "        \"Horário de Check-out\": horario_checkout,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3880f",
   "metadata": {},
   "source": [
    "## 4. Colocando o Robô para Trabalhar: O Processo de Extração\n",
    "\n",
    "Com o \"cérebro\" do nosso robô (`extrair_detalhes_anuncio`) devidamente construído, é hora de orquestrar a operação completa. A seção a seguir irá:\n",
    "1.  Definir os arquivos de trabalho.\n",
    "2.  Carregar a lista de \"tarefas\" (os links a serem visitados).\n",
    "3.  Ligar o navegador.\n",
    "4.  Executar o loop que passará por cada tarefa, uma por uma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea8754",
   "metadata": {},
   "source": [
    "### 4.1. Definindo o Alvo e os Arquivos de Saída\n",
    "\n",
    "O primeiro passo é dizer ao robô qual arquivo contém a lista de links para visitar. A partir desse nome, ele irá gerar automaticamente os nomes para o arquivo de resultados parciais (o \"backup\" do progresso) e o arquivo de saída final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea347c7",
   "metadata": {},
   "source": [
    "### 4.2. Carregamento e Preparação dos Dados de Entrada\n",
    "Lemos o arquivo de entrada e preparamos a lista de imóveis únicos a serem processados. Também verificamos se já existe um arquivo de resultados parciais para continuar o trabalho de onde parou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d056aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURAÇÃO MANUAL ---\n",
    "# ATENÇÃO: Altere a linha abaixo para indicar o caminho e nome do seu arquivo CSV de entrada.\n",
    "input_filename = 'dados/airbnb_dados_gerais_1_hospede_4_noites2025_07_08.csv'\n",
    "\n",
    "# --- GERAÇÃO AUTOMÁTICA DOS NOMES DE SAÍDA ---\n",
    "# Pega o nome do arquivo de entrada sem a extensão .csv.\n",
    "base_name, extension = os.path.splitext(input_filename)\n",
    "# Cria o nome do arquivo para salvar os resultados parciais (ex: 'meus_dados_parciais.csv').\n",
    "partial_results_filename = f\"{base_name}_resultados_parciais.csv\"\n",
    "# Cria o nome do arquivo final que conterá todos os dados combinados.\n",
    "final_output_filename = f\"{base_name}_completo.csv\"\n",
    "\n",
    "# Imprime os nomes dos arquivos para verificação.\n",
    "print(f\"Arquivo de Entrada: {input_filename}\")\n",
    "print(f\"Arquivo de Backup (Resultados Parciais): {partial_results_filename}\")\n",
    "print(f\"Arquivo de Saída Final: {final_output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533f51a",
   "metadata": {},
   "source": [
    "### 4.2. Carregando a Lista de Tarefas e Verificando Trabalhos Anteriores\n",
    "\n",
    "Agora, o código lê o arquivo de entrada. Mas, antes de começar o trabalho pesado, ele faz uma verificação inteligente: procura pelo arquivo de resultados parciais. \n",
    "\n",
    "Se ele existir, o robô é inteligente o suficiente para ler o que já foi feito e pular os anúncios que já visitou, economizando um tempo valioso e permitindo que o trabalho continue de onde parou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d63b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenta carregar o arquivo CSV de entrada.\n",
    "try:\n",
    "    df_airbnb = pd.read_csv(input_filename)\n",
    "    print(f\"\\nArquivo '{input_filename}' carregado com sucesso. Total de {len(df_airbnb)} linhas.\")\n",
    "# Se o arquivo não for encontrado, exibe uma mensagem de erro crítica.\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRO CRÍTICO: O arquivo de entrada '{input_filename}' não foi encontrado. Verifique o nome e o caminho.\")\n",
    "    # sys.exit(1) # Este comando pararia o script; em um notebook, o ideal é apenas exibir o erro.\n",
    "\n",
    "# Valida se o DataFrame carregado contém as colunas essenciais para o script funcionar.\n",
    "if 'ID Imóvel' not in df_airbnb.columns or 'Link' not in df_airbnb.columns:\n",
    "    print(\"ERRO CRÍTICO: O arquivo CSV de entrada precisa ter as colunas 'ID Imóvel' e 'Link'.\")\n",
    "    # sys.exit(1) \n",
    "\n",
    "# Prepara a lista de tarefas do robô em um novo DataFrame:\n",
    "# 1. Seleciona apenas as colunas 'ID Imóvel' e 'Link'.\n",
    "# 2. Remove todas as linhas que são duplicatas com base na coluna 'ID Imóvel', para processar cada anúncio apenas uma vez.\n",
    "# 3. Remove quaisquer linhas onde a coluna 'Link' seja nula (NaN), pois não seria possível visitá-las.\n",
    "df_para_scrape = df_airbnb[['ID Imóvel', 'Link']].drop_duplicates(subset=['ID Imóvel']).dropna(subset=['Link'])\n",
    "total_links_unicos = len(df_para_scrape)\n",
    "print(f\"Encontrados {total_links_unicos} imóveis únicos para processar.\")\n",
    "\n",
    "# Lógica para retomar o trabalho:\n",
    "# Inicializa um 'set' (conjunto) vazio para armazenar os IDs já processados. Um 'set' é mais rápido que uma lista para verificações.\n",
    "processed_ids = set() \n",
    "# Verifica se o arquivo de backup já existe no diretório.\n",
    "if os.path.exists(partial_results_filename):\n",
    "    print(f\"Encontrado arquivo de backup de resultados parciais: '{partial_results_filename}'.\")\n",
    "    # Se existe, carrega o arquivo.\n",
    "    df_parcial = pd.read_csv(partial_results_filename)\n",
    "    # Garante que o arquivo de backup tenha a coluna 'ID Imóvel' para evitar erros.\n",
    "    if 'ID Imóvel' in df_parcial.columns:\n",
    "        # Adiciona todos os IDs do arquivo de backup ao nosso conjunto de IDs já processados.\n",
    "        processed_ids = set(df_parcial['ID Imóvel'])\n",
    "        print(f\"Retomando trabalho. {len(processed_ids)} imóveis já foram processados e serão pulados.\")\n",
    "    else:\n",
    "        # Caso o arquivo parcial esteja corrompido ou sem a coluna necessária.\n",
    "        print(\"AVISO: O arquivo de resultados parciais não contém a coluna 'ID Imóvel' e será ignorado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8bf424",
   "metadata": {},
   "source": [
    "### 4.3. Ligando os Motores: Inicialização do Navegador\n",
    "\n",
    "É hora de iniciar o Selenium. Para máxima eficiência, operamos o navegador em \"modo fantasma\" (headless), ou seja, sem abrir uma janela visual. Também o instruímos a não carregar imagens e folhas de estilo (CSS), tornando a navegação muito mais rápida e focada apenas no que importa: os dados em HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9818d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um objeto de configuração para o navegador Firefox.\n",
    "options = Options()\n",
    "# Adiciona o argumento para rodar o navegador em modo headless (em segundo plano, sem janela visível).\n",
    "options.add_argument('--headless')\n",
    "# Desabilita o carregamento de imagens (valor 2) para acelerar o carregamento da página.\n",
    "options.set_preference(\"permissions.default.image\", 2)\n",
    "# Desabilita o carregamento de arquivos de estilo (CSS), o que também acelera a navegação.\n",
    "options.set_preference(\"permissions.default.stylesheet\", 2)\n",
    "# Desabilita o download de fontes customizadas da web.\n",
    "options.set_preference(\"gfx.downloadable_fonts.enabled\", False)\n",
    "\n",
    "# Imprime uma mensagem para o usuário saber que o navegador está sendo iniciado.\n",
    "print(\"\\nIniciando o navegador Firefox em modo headless...\")\n",
    "# Inicia o driver do Firefox com todas as opções que configuramos. O robô está pronto para a ação.\n",
    "driver = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc416c16",
   "metadata": {},
   "source": [
    "### 4.4. A Maratona de Extração: O Loop Principal\n",
    "\n",
    "Esta é a célula onde a mágica acontece. O robô começa sua jornada, passando por cada anúncio da lista, um por um. Ele chama a função que construímos anteriormente (`extrair_detalhes_anuncio`) para cada link, coleta os detalhes e os salva imediatamente no arquivo de backup.\n",
    "\n",
    "Para garantir que ele não se \"canse\" (ou consuma muita memória durante longas extrações), nós o programamos para reiniciar a si mesmo a cada 50 visitas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaaad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contador para saber quantos itens foram processados nesta sessão específica do script.\n",
    "processed_in_this_session = 0\n",
    "# Calcula o número total de itens que realmente precisam ser processados, subtraindo os que já foram feitos.\n",
    "total_a_processar = total_links_unicos - len(processed_ids)\n",
    "print(f\"\\nIniciando a extração para {total_a_processar} novos anúncios únicos...\")\n",
    "\n",
    "# Itera sobre cada linha do DataFrame que contém a lista de tarefas (links).\n",
    "for index, row in df_para_scrape.iterrows():\n",
    "    # Extrai o ID e a URL da linha atual.\n",
    "    id_imovel = row['ID Imóvel']\n",
    "    url = row['Link']\n",
    "\n",
    "    # Lógica de retomada: se o ID do imóvel atual já está no conjunto de IDs processados, pula para o próximo.\n",
    "    if id_imovel in processed_ids:\n",
    "        continue # A palavra-chave 'continue' interrompe a iteração atual e vai para a próxima.\n",
    "\n",
    "    # Lógica de reinicialização: a cada 50 itens, fecha e abre o navegador para limpar a memória RAM.\n",
    "    # O operador '%' (módulo) retorna o resto de uma divisão. Se o resto de processed_in_this_session / 50 for 0, reinicia.\n",
    "    if processed_in_this_session > 0 and processed_in_this_session % 50 == 0:\n",
    "        print(f\"\\n--- Processados {processed_in_this_session} links nesta sessão. Reiniciando o navegador... ---\")\n",
    "        driver.quit() # Fecha completamente o navegador atual.\n",
    "        time.sleep(5) # Pausa de 5 segundos para garantir que o processo do navegador foi totalmente encerrado.\n",
    "        driver = webdriver.Firefox(options=options) # Inicia uma nova instância do navegador com as mesmas configurações.\n",
    "        print(\"--- Navegador reiniciado. Continuando a extração... ---\\n\")\n",
    "\n",
    "    # Imprime o progresso atual para acompanhamento.\n",
    "    print(f\"Processando anúncio {processed_in_this_session + 1}/{total_a_processar} (ID: {id_imovel})\")\n",
    "\n",
    "    # Antes de chamar a função, verifica se a URL é válida (não é nula, é um texto e começa com http).\n",
    "    if pd.notna(url) and isinstance(url, str) and url.startswith(\"http\"):\n",
    "        # Chama a função principal para extrair os detalhes da página, passando o driver e a url.\n",
    "        detalhes = extrair_detalhes_anuncio(driver, url)\n",
    "    else:\n",
    "        # Se a URL for inválida, registra um dicionário de erro para não quebrar o processo.\n",
    "        print(f\"Link inválido ou ausente para o ID Imóvel {id_imovel}. Pulando.\")\n",
    "        detalhes = {\n",
    "            \"Quartos\": 'Link Inválido', \"Camas\": 'Link Inválido',\n",
    "            \"Banheiros\": 'Link Inválido', \"Horário de Check-in\": 'Link Inválido',\n",
    "            \"Horário de Check-out\": 'Link Inválido'\n",
    "        }\n",
    "\n",
    "    # Adiciona o ID do imóvel ao dicionário de resultados para garantir a correspondência correta na hora de salvar.\n",
    "    detalhes['ID Imóvel'] = id_imovel\n",
    "    # Converte o dicionário de resultado em um pequeno DataFrame de uma linha.\n",
    "    df_resultado_atual = pd.DataFrame([detalhes])\n",
    "\n",
    "    # Lógica para escrever o cabeçalho (nomes das colunas) no arquivo CSV apenas na primeira vez que o arquivo é criado.\n",
    "    escrever_header = not os.path.exists(partial_results_filename)\n",
    "\n",
    "    # Anexa (append) o resultado atual ao arquivo CSV de backup. 'mode='a'' significa 'append' (adicionar ao final).\n",
    "    df_resultado_atual.to_csv(\n",
    "        partial_results_filename,\n",
    "        mode='a',\n",
    "        header=escrever_header,\n",
    "        index=False # Não salva o índice do DataFrame no arquivo.\n",
    "    )\n",
    "    \n",
    "    # Incrementa o contador de itens processados nesta sessão.\n",
    "    processed_in_this_session += 1\n",
    "\n",
    "print(\"\\nExtração de todos os novos links concluída.\")\n",
    "# Ao final de todo o processo, fecha a última instância do navegador para liberar os recursos do sistema.\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e85bb",
   "metadata": {},
   "source": [
    "## 5. A Etapa Final: Unificando os Dados\n",
    "\n",
    "Com a maratona de extração concluída, o robô agora tem dois arquivos importantes: o CSV original com as informações básicas e o CSV de resultados parciais com todos os novos detalhes.\n",
    "\n",
    "A tarefa final é juntar esses dois arquivos. Usaremos a coluna `ID Imóvel` como a \"chave\" para combinar as informações, garantindo que os detalhes de quartos, camas e banheiros sejam adicionados à linha correta do anúncio correspondente. O resultado é um único arquivo CSV, completo e enriquecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f258d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMapeando dados extraídos de volta para o DataFrame completo...\")\n",
    "\n",
    "# Verifica se o arquivo de resultados parciais foi realmente criado (ou seja, se o loop de extração rodou pelo menos uma vez).\n",
    "if os.path.exists(partial_results_filename):\n",
    "    # Carrega todos os detalhes que foram extraídos e salvos no arquivo de backup.\n",
    "    df_detalhes = pd.read_csv(partial_results_filename)\n",
    "\n",
    "    # Define os nomes das colunas que foram adicionadas pelo scraper.\n",
    "    colunas_novas = ['Quartos', 'Camas', 'Banheiros', 'Horário de Check-in', 'Horário de Check-out']\n",
    "\n",
    "    # Remove essas colunas do DataFrame original, caso elas já existam de uma execução anterior.\n",
    "    # 'errors='ignore'' evita que o programa dê um erro caso as colunas não existam para serem removidas.\n",
    "    df_airbnb_sem_detalhes = df_airbnb.drop(columns=colunas_novas, errors='ignore')\n",
    "\n",
    "    # A mágica da unificação: usa a função 'merge' do pandas para juntar os dois DataFrames.\n",
    "    # Funciona como um PROCV/VLOOKUP no Excel, usando o 'ID Imóvel' como a chave de correspondência.\n",
    "    # 'how='left'' garante que todos os anúncios do arquivo original (df_airbnb_sem_detalhes) sejam mantidos no resultado final.\n",
    "    df_final = pd.merge(df_airbnb_sem_detalhes, df_detalhes, on='ID Imóvel', how='left')\n",
    "\n",
    "    try:\n",
    "        # Tenta salvar o DataFrame final e unificado em um novo arquivo CSV. 'index=False' evita salvar o índice do DataFrame como uma coluna.\n",
    "        df_final.to_csv(final_output_filename, index=False, encoding='utf-8-sig') # 'utf-8-sig' ajuda na compatibilidade com Excel.\n",
    "        print(f\"\\nSUCESSO! DataFrame final com {len(df_final)} linhas salvo em '{final_output_filename}'\")\n",
    "    except Exception as e:\n",
    "        # Captura possíveis erros durante o salvamento do arquivo (ex: falta de permissão).\n",
    "        print(f\"\\nOcorreu um erro ao salvar o arquivo CSV final: {e}\")\n",
    "else:\n",
    "    # Mensagem para o caso de nenhum novo item ter sido processado nesta execução.\n",
    "    print(\"\\nNenhum dado novo foi extraído. O arquivo final não foi gerado.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
